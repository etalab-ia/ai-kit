{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6471e25",
   "metadata": {},
   "source": [
    "## Robustness Testing (Article 15)\n",
    "\n",
    "### Adversarial Robustness\n",
    "\n",
    "Test model resilience to adversarial inputs:\n",
    "\n",
    "- [ ] Small perturbations to inputs\n",
    "- [ ] Out-of-distribution samples\n",
    "- [ ] Edge cases and boundary conditions\n",
    "\n",
    "### Data Distribution Shifts\n",
    "\n",
    "Test performance under distribution shifts:\n",
    "\n",
    "- [ ] Temporal shifts (data from different time periods)\n",
    "- [ ] Geographic shifts (data from different regions)\n",
    "- [ ] Demographic shifts (different population characteristics)\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "Systematic analysis of failure modes:\n",
    "\n",
    "- [ ] Confusion matrix analysis\n",
    "- [ ] Error patterns by input characteristics\n",
    "- [ ] Failure case documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness Testing\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.ylabel('True Label')\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.show()\n",
    "\n",
    "# 2. Error Analysis\n",
    "# errors = X_test[y_test != y_pred]\n",
    "# print(f\"Total errors: {len(errors)}\")\n",
    "# print(f\"Error rate: {len(errors) / len(y_test):.2%}\")\n",
    "\n",
    "# 3. Performance by Confidence\n",
    "# confidence = y_pred_proba.max(axis=1)\n",
    "# confidence_bins = pd.cut(confidence, bins=[0, 0.6, 0.8, 0.9, 1.0])\n",
    "#\n",
    "# print(\"\\nPerformance by Confidence Level:\")\n",
    "# for bin_range in confidence_bins.cat.categories:\n",
    "#     mask = confidence_bins == bin_range\n",
    "#     if mask.sum() > 0:\n",
    "#         acc = accuracy_score(y_test[mask], y_pred[mask])\n",
    "#         print(f\"  {bin_range}: {acc:.3f} ({mask.sum()} samples)\")\n",
    "\n",
    "print(\"Robustness testing complete. Document findings above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Model Evaluation: GPT-4 Baseline for French Government FAQ\n",
    "\n",
    "**Category**: evaluations\n",
    "**Purpose**: Establish baseline performance metrics for GPT-4 on French government FAQ dataset before fine-tuning\n",
    "**Author**: Data Science Team\n",
    "**Created**: 2024-10-15\n",
    "\n",
    "**Data Sources**:\n",
    "- Training data: `data/faq-training-v2.csv` (10,000 samples, 2023-2024)\n",
    "- Validation data: `data/faq-validation-v2.csv` (2,000 samples)\n",
    "- Test data: `data/faq-test-v2.csv` (1,000 samples, held out)\n",
    "- Baseline results: `results/gpt-3.5-baseline.json` (previous baseline)\n",
    "\n",
    "**Dependencies**:\n",
    "```\n",
    "# Model evaluation dependencies\n",
    "scikit-learn>=1.3.0\n",
    "numpy>=1.24.0\n",
    "pandas>=2.1.0\n",
    "matplotlib>=3.8.0\n",
    "seaborn>=0.13.0\n",
    "openai>=1.3.0  # For GPT-4 API\n",
    "```\n",
    "\n",
    "**Model Requirements**:\n",
    "- Model: GPT-4-turbo (gpt-4-1106-preview)\n",
    "- API version: 2024-02-01\n",
    "- Temperature: 0.7\n",
    "- Max tokens: 500\n",
    "\n",
    "**EU AI Act Context**:\n",
    "- System risk level: High-risk (essential public services)\n",
    "- Intended purpose: Automated FAQ responses for French government services\n",
    "- Target performance: \n",
    "  - Accuracy ≥ 90% (correct answer identification)\n",
    "  - Hallucination rate < 5% (factually incorrect responses)\n",
    "  - Response time < 3 seconds (95th percentile)\n",
    "  - French language quality ≥ 4.0/5.0 (human evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1939780",
   "metadata": {},
   "source": [
    "## EU AI Act Evaluation Requirements\n",
    "\n",
    "### Article 15: Accuracy Requirements\n",
    "\n",
    "For high-risk AI systems, demonstrate:\n",
    "- [ ] Appropriate level of accuracy\n",
    "- [ ] Robustness against errors\n",
    "- [ ] Resilience to manipulation\n",
    "- [ ] Cybersecurity measures\n",
    "\n",
    "### Fairness and Bias Assessment\n",
    "\n",
    "Required for systems affecting individuals:\n",
    "- [ ] Performance across demographic groups\n",
    "- [ ] Disparate impact analysis\n",
    "- [ ] Bias mitigation effectiveness\n",
    "- [ ] Fairness metrics documented\n",
    "\n",
    "### Validation Methodology\n",
    "\n",
    "- [ ] Independent test set (not used in training)\n",
    "- [ ] Representative of deployment conditions\n",
    "- [ ] Edge cases and boundary conditions tested\n",
    "- [ ] Methodology documented and reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "\n",
    "# Load test dataset\n",
    "# test_df = pd.read_csv('data/test-dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model inference on test set\n",
    "# predictions = model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EU AI Act Evaluation Metrics\n",
    "\n",
    "\n",
    "# Load model and test data\n",
    "# model = load_model('path/to/model')\n",
    "# X_test = pd.read_csv('data/test-features.csv')\n",
    "# y_test = pd.read_csv('data/test-labels.csv')\n",
    "\n",
    "# Make predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Core Performance Metrics (Article 15)\n",
    "metrics = {\n",
    "    \"accuracy\": 0.0,  # accuracy_score(y_test, y_pred)\n",
    "    \"precision\": 0.0,  # precision_score(y_test, y_pred, average='weighted')\n",
    "    \"recall\": 0.0,  # recall_score(y_test, y_pred, average='weighted')\n",
    "    \"f1_score\": 0.0,  # f1_score(y_test, y_pred, average='weighted')\n",
    "    \"roc_auc\": 0.0,  # roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "}\n",
    "\n",
    "print(\"Core Performance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Target thresholds (define based on use case)\n",
    "thresholds = {\n",
    "    \"accuracy\": 0.90,  # Minimum 90% accuracy\n",
    "    \"precision\": 0.85,  # Minimum 85% precision\n",
    "    \"recall\": 0.85,  # Minimum 85% recall\n",
    "    \"f1_score\": 0.85,  # Minimum 85% F1\n",
    "}\n",
    "\n",
    "print(\"\\nThreshold Compliance:\")\n",
    "for metric, threshold in thresholds.items():\n",
    "    status = \"✓ PASS\" if metrics[metric] >= threshold else \"✗ FAIL\"\n",
    "    print(f\"  {metric}: {metrics[metric]:.4f} >= {threshold:.4f} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f964533",
   "metadata": {},
   "source": [
    "## Fairness and Bias Assessment\n",
    "\n",
    "### Demographic Parity\n",
    "\n",
    "Evaluate if model predictions are independent of protected attributes:\n",
    "\n",
    "```python\n",
    "# For each protected attribute (e.g., gender, age, ethnicity)\n",
    "# Calculate: P(ŷ=1 | A=a) for each group a\n",
    "```\n",
    "\n",
    "**Acceptable threshold**: Ratio between groups should be > 0.8\n",
    "\n",
    "### Equal Opportunity\n",
    "\n",
    "Evaluate if true positive rates are similar across groups:\n",
    "\n",
    "```python\n",
    "# Calculate: TPR(A=a) = P(ŷ=1 | y=1, A=a) for each group\n",
    "```\n",
    "\n",
    "**Acceptable threshold**: Difference between groups should be < 0.1\n",
    "\n",
    "### Equalized Odds\n",
    "\n",
    "Evaluate if both TPR and FPR are similar across groups:\n",
    "\n",
    "```python\n",
    "# Calculate both:\n",
    "# TPR(A=a) = P(ŷ=1 | y=1, A=a)\n",
    "# FPR(A=a) = P(ŷ=1 | y=0, A=a)\n",
    "```\n",
    "\n",
    "**Acceptable threshold**: Differences should be < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "# accuracy = accuracy_score(test_df['label'], predictions)\n",
    "# f1 = f1_score(test_df['label'], predictions, average='weighted')\n",
    "# print(f'Accuracy: {accuracy:.3f}')\n",
    "# print(f'F1 Score: {f1:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ad7a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness Metrics Implementation\n",
    "\n",
    "\n",
    "def calculate_fairness_metrics(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"Calculate fairness metrics for a protected attribute.\"\"\"\n",
    "    groups = protected_attribute.unique()\n",
    "    metrics = {}\n",
    "\n",
    "    for group in groups:\n",
    "        mask = protected_attribute == group\n",
    "        y_true_group = y_true[mask]\n",
    "        y_pred_group = y_pred[mask]\n",
    "\n",
    "        # Demographic parity: P(ŷ=1 | A=a)\n",
    "        positive_rate = (y_pred_group == 1).mean()\n",
    "\n",
    "        # Equal opportunity: TPR\n",
    "        tp = ((y_true_group == 1) & (y_pred_group == 1)).sum()\n",
    "        fn = ((y_true_group == 1) & (y_pred_group == 0)).sum()\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        # False positive rate for equalized odds\n",
    "        fp = ((y_true_group == 0) & (y_pred_group == 1)).sum()\n",
    "        tn = ((y_true_group == 0) & (y_pred_group == 0)).sum()\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "        metrics[group] = {\n",
    "            \"positive_rate\": positive_rate,\n",
    "            \"tpr\": tpr,\n",
    "            \"fpr\": fpr,\n",
    "            \"sample_size\": len(y_true_group),\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Example usage (uncomment when data available):\n",
    "# protected_attr = X_test['gender']  # or age_group, ethnicity, etc.\n",
    "# fairness_results = calculate_fairness_metrics(y_test, y_pred, protected_attr)\n",
    "#\n",
    "# print(\"Fairness Metrics by Group:\")\n",
    "# for group, metrics in fairness_results.items():\n",
    "#     print(f\"\\n{group}:\")\n",
    "#     print(f\"  Positive rate: {metrics['positive_rate']:.3f}\")\n",
    "#     print(f\"  TPR: {metrics['tpr']:.3f}\")\n",
    "#     print(f\"  FPR: {metrics['fpr']:.3f}\")\n",
    "#     print(f\"  Sample size: {metrics['sample_size']}\")\n",
    "\n",
    "# Calculate disparate impact ratio\n",
    "# max_rate = max(m['positive_rate'] for m in fairness_results.values())\n",
    "# min_rate = min(m['positive_rate'] for m in fairness_results.values())\n",
    "# disparate_impact = min_rate / max_rate if max_rate > 0 else 0\n",
    "# print(f\"\\nDisparate Impact Ratio: {disparate_impact:.3f}\")\n",
    "# print(f\"Status: {'✓ PASS' if disparate_impact > 0.8 else '✗ FAIL'} (threshold: 0.8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "**Core Metrics**:\n",
    "- Accuracy: [X%] (Target: ≥Y%)\n",
    "- Precision: [X%] (Target: ≥Y%)\n",
    "- Recall: [X%] (Target: ≥Y%)\n",
    "- F1 Score: [X] (Target: ≥Y)\n",
    "\n",
    "**Fairness Assessment**:\n",
    "- Disparate Impact: [X] (Target: >0.8)\n",
    "- Equal Opportunity: [Max difference: X] (Target: <0.1)\n",
    "- Protected attributes analyzed: [List]\n",
    "\n",
    "**Robustness**:\n",
    "- Adversarial testing: [Pass/Fail]\n",
    "- Distribution shift testing: [Pass/Fail]\n",
    "- Edge case handling: [Pass/Fail]\n",
    "\n",
    "### EU AI Act Compliance\n",
    "\n",
    "- [ ] Accuracy requirements met (Article 15)\n",
    "- [ ] Fairness requirements met\n",
    "- [ ] Robustness demonstrated\n",
    "- [ ] Validation methodology documented\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "- [ ] **Approved for production**: All requirements met\n",
    "- [ ] **Approved with monitoring**: Meets requirements, recommend ongoing monitoring\n",
    "- [ ] **Not approved**: Requires improvements (list below)\n",
    "\n",
    "**Required improvements** (if any):\n",
    "1. [Improvement 1]\n",
    "2. [Improvement 2]\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **If approved**: Create compliance documentation in `notebooks/compliance/`\n",
    "2. **Tag this evaluation**: \n",
    "   ```bash\n",
    "   just notebook tag notebooks/evaluations/[this-notebook].ipynb \\\n",
    "     --identifier [model]-[version]-eval \\\n",
    "     --message \"Evaluation approved by [Your Name]\"\n",
    "   ```\n",
    "3. **Reference in deployment**: Link to this evaluation in deployment documentation\n",
    "\n",
    "**Evaluation Date**: [YYYY-MM-DD]\n",
    "**Evaluator**: [Your Name]\n",
    "**Next Evaluation**: [YYYY-MM-DD] (recommended: quarterly for high-risk systems)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
